{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обогащение имеющегося датасета новыми данными"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вступительная база"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорт необходимых библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функции, испольщуемые далее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_custom(df1, df2, month = False, day = False, hour = False, Q = False):\n",
    "    on_list = ['year']\n",
    "    if hour:\n",
    "        month, day = True, True\n",
    "        on_list.append('hour')\n",
    "    if day:\n",
    "        month = True\n",
    "        on_list.append('day')\n",
    "    if month:\n",
    "        on_list.append('month')\n",
    "    if Q:\n",
    "        on_list.append('Q')\n",
    "    return df1.merge(df2, on=on_list, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Считаем датасет и получим новые солбцы с временем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/test.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_date(df, date_row, day=False, hour = False, Q = False):\n",
    "    df1 = df.copy()\n",
    "    df1['year'] = df1[date_row].dt.year\n",
    "    df1['month'] = df1[date_row].dt.month\n",
    "    if hour:\n",
    "        day = True\n",
    "        df1['hour'] = df1[date_row].dt.hour\n",
    "    if day:\n",
    "        df1['day'] = df1[date_row].dt.day\n",
    "    if Q:\n",
    "        df1['Q'] = df1[date_row].dt.quarter\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = split_date(df, 'date', hour=True, Q=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Добавим в датасет праздничные и выходные дни"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_url(year):\n",
    "    return f\"https://www.work-day.co.uk/data_excel?from={year}-01-01&to={year}-12-31&step=1&cols=52,0,1;52,0,3;52,0,11;52,0,12;52,0,2;&country_code_adding=england\"\n",
    "\n",
    "def fetch_table_data(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        table = soup.find('table', id='data_table')\n",
    "        if table:\n",
    "            table_html = StringIO(str(table))\n",
    "            return pd.read_html(table_html)[0]\n",
    "        else:\n",
    "            print(f\"Таблица не найдена на странице: {url}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Не удалось получить данные: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def get_data_for_years(start_year, end_year):\n",
    "    all_data = pd.DataFrame()\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        url = generate_url(year)\n",
    "        year_data = fetch_table_data(url)\n",
    "        if year_data is not None:\n",
    "            all_data = pd.concat([all_data, year_data], ignore_index=True)\n",
    "    return all_data\n",
    "\n",
    "def rename_columns(data):\n",
    "    \"\"\"Переименовывает столбцы в DataFrame.\"\"\"\n",
    "    rename_map = {\n",
    "        'Export to Excel $ 1.99': 'date',\n",
    "        '× England, working days': 'woring_days',\n",
    "        '× England, public holidays': 'public_holidays',\n",
    "        '× England, work hours (h)': 'work_hours',\n",
    "        '× England, wages (£)': 'wages',\n",
    "        '× England, weekend days': 'weekend_days'\n",
    "    }\n",
    "    return data.rename(columns=rename_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data_for_years(1979, 2020)\n",
    "data = rename_columns(data)\n",
    "data = data.drop(['work_hours', 'wages'], axis = 1) # useless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_date(df):\n",
    "    data = df.copy()\n",
    "    data[['day_of_week', 'day', 'month', 'year']] = data['date'].str.extract(\n",
    "        r'(?P<day_of_week>\\w+),\\s(?P<day>\\d+)\\s(?P<month>\\w+),\\s(?P<year>\\d{4})'\n",
    "    )\n",
    "    \n",
    "    data['day'] = data['day'].astype(int)\n",
    "    data['year'] = data['year'].astype(int)\n",
    "    \n",
    "    day_of_week_map = {\n",
    "        'Mon': 1, 'Tue': 2, 'Wed': 3, 'Thu': 4, 'Fri': 5, 'Sat': 6, 'Sun': 7\n",
    "    }\n",
    "    \n",
    "    month_map = {\n",
    "        'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,\n",
    "        'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12\n",
    "    }\n",
    "    \n",
    "    data['day_of_week'] = data['day_of_week'].map(day_of_week_map)\n",
    "    data['month'] = data['month'].map(month_map)\n",
    "    \n",
    "    data = data.drop('date', axis = 1)\n",
    "    return data\n",
    "data = clear_date(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merge_custom(df, data, day=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Добавим статистику по лицензированным таксистам в каждый год"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi = pd.ExcelFile(\"data/taxi0101.ods\")\n",
    "taxi.close()\n",
    "taxi = taxi.parse('TAXI0101a')[['Year', 'London licensed taxis (thousands) [note 1]']]\n",
    "taxi['London licensed taxis (thousands) [note 1]'] = taxi['London licensed taxis (thousands) [note 1]']*1000\n",
    "taxi = taxi.rename(columns = {'London licensed taxis (thousands) [note 1]':'licensed taxis', 'Year':'year'})\n",
    "df = merge_custom(df, taxi)\n",
    "# df['licensed taxis'].fillna(df['licensed taxis'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Добавим статистику по количеству ДТП"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# casualty = pd.read_csv('data/dft-road-casualty-statistics-casualty-1979-latest-published-year.csv') #здесь можно найти насколько пострадали участники дтп\n",
    "collision = pd.read_csv('data/dft-road-casualty-statistics-collision-1979-latest-published-year.csv')\n",
    "vehicle = pd.read_csv('data/dft-road-casualty-statistics-vehicle-1979-latest-published-year.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#нас интересуют только эти столбцы\n",
    "collision = collision[['accident_index', 'date', 'local_authority_district', 'time' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# коды районов лондона\n",
    "london_boroughs = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,57,570]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "collision = collision[collision['local_authority_district'].isin(london_boroughs)]\n",
    "#получили индексы аварий, которые были совершены в лондоне, их дату и время"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# нас инетересуют только эти столбцы\n",
    "vehicle = vehicle[['accident_index', 'vehicle_type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vehicle_type == 1 - велосипед\n",
    "cycle_collision = collision.merge(vehicle[vehicle['vehicle_type'] == 1], on='accident_index') #количество дтп с велосипедистами\n",
    "all_collision = collision #все дтп\n",
    "\n",
    "\n",
    "#группируем все дтп по 24 часа\n",
    "cycle_collision['datetime'] = pd.to_datetime(cycle_collision['date'] + ' ' + cycle_collision['time'], dayfirst=True)\n",
    "all_collision['datetime'] = pd.to_datetime(all_collision['date'] + ' ' + all_collision['time'], dayfirst=True)\n",
    "\n",
    "cycle_collision = cycle_collision['datetime'].to_frame().sort_values('datetime')\n",
    "all_collision = all_collision['datetime'].to_frame().sort_values('datetime')\n",
    "\n",
    "cycle_collision = cycle_collision.set_index('datetime').resample('h').size().rolling('24h').sum().to_frame(name=\"collision_cycle_count\").reset_index()\n",
    "all_collision = all_collision.set_index('datetime').resample('h').size().rolling('24h').sum().to_frame(name=\"collision_all_count\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle_collision = cycle_collision.rename(columns={'datetime':'date'})\n",
    "all_collision = all_collision.rename(columns={'datetime':'date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(cycle_collision, on='date', how='left')\n",
    "df = df.merge(all_collision, on='date', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#наны только в начале, так что заполним их нулями\n",
    "df['collision_cycle_count'].fillna(0, inplace=True)\n",
    "df['collision_all_count'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claimant\n",
    "ежемесячный процент заявителей в лондоне по месяцам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "claimant = pd.read_excel('data/Claimant Count  E12000007 London  People  SA  Percentage (%).xls')\n",
    "claimant['date'] = pd.to_datetime(claimant['year'])\n",
    "claimant = split_date(claimant, 'date')\n",
    "claimant = claimant.drop(columns='date')\n",
    "\n",
    "df = merge_custom(df, claimant, month=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPIH на мотоциклы и велосипедисты\n",
    "ежемесячные данные с января 1988"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "motorcycles_and_bicycles = pd.read_excel('data/CPIH INDEX 07.1.23 Motorcycles and bicycles 2015=100.xls')\n",
    "motorcycles_and_bicycles['date'] = pd.to_datetime(motorcycles_and_bicycles['year'])\n",
    "motorcycles_and_bicycles = split_date(motorcycles_and_bicycles, 'date')\n",
    "motorcycles_and_bicycles = motorcycles_and_bicycles.drop(columns='date')\n",
    "\n",
    "df = merge_custom(df, motorcycles_and_bicycles, month=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GDP UK\n",
    "ежеквартальные данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp = pd.read_csv('data/GDP_UK.csv')\n",
    "gdp[['year', 'Q']] = gdp['year'].str.split(' ', expand=True)\n",
    "gdp['year'] = gdp['year'].astype(int)\n",
    "gdp['Q'] = gdp['Q'].str.replace('Q', '').astype(int)\n",
    "\n",
    "df = merge_custom(df, gdp, Q=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Инфляция UK\n",
    "ежемесячные данные с января 1989 года"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflation = pd.read_csv('data/Inflation_UK.csv')\n",
    "inflation['date'] = pd.to_datetime(inflation['year'])\n",
    "inflation = split_date(inflation, 'date')\n",
    "inflation = inflation.drop(columns=['date'])\n",
    "\n",
    "df = merge_custom(df, inflation, month=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Economic activity rate\n",
    "ежемесячные данные с апреля 1992"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_rate = pd.read_excel('data/LFS Economic activity rate London Aged 16-64 All % SA.xls')\n",
    "activity_rate['date'] = pd.to_datetime(activity_rate['year'])\n",
    "activity_rate = split_date(activity_rate, 'date')\n",
    "activity_rate = activity_rate.drop(columns=['date'])\n",
    "\n",
    "df = merge_custom(df, activity_rate, month=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Количество работающих \n",
    "ежемесячные данные с апреля 1992"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_employment = pd.read_csv('data/LFS In employment London All Thousands SA.csv')\n",
    "\n",
    "in_employment['date'] = pd.to_datetime(in_employment['year'])\n",
    "in_employment = split_date(in_employment, 'date')\n",
    "in_employment = in_employment.drop(columns=['date'])\n",
    "\n",
    "df = merge_custom(df, in_employment, month=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Заполненность Темзы\n",
    "ежедневные данные с января 1989"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Данные с января 89 года\n",
    "reservoir_level = pd.read_excel('data/london-reservoir-levels.xlsx', sheet_name='Daily')\n",
    "reservoir_level = split_date(reservoir_level, 'Date', day=True)\n",
    "reservoir_level = reservoir_level.drop(columns=['Lee_level_capacity', 'Date'])\n",
    "df = merge_custom(df, reservoir_level, day=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Популяция\n",
    "по кварталам с апреля 1984, по месяцам с апреля 1992"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = pd.read_csv('data/Population aged 16 and over London All Thousands NSA.csv')\n",
    "population['date'] = pd.to_datetime(population['year'])\n",
    "population = split_date(population, 'date')\n",
    "population = population.drop(columns = 'date')\n",
    "\n",
    "df = merge_custom(df, population, month=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RPI на алкоголь и табак\n",
    "ежемесячные данные с февраля 1987"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#данные с февраля 1987 года\n",
    "alco_and_tobacco = pd.read_csv('data/RPIPercentage change over 1 month - Alcohol and tobacco.csv')\n",
    "alco_and_tobacco['date'] = pd.to_datetime(alco_and_tobacco['year'])\n",
    "alco_and_tobacco = split_date(alco_and_tobacco, 'date')\n",
    "alco_and_tobacco = alco_and_tobacco.drop(columns = 'date')\n",
    "\n",
    "df = merge_custom(df, alco_and_tobacco, month=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Процент безработных\n",
    "ежемесячные данные с апреля 1992"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#данные с апреля 1992\n",
    "unemployment_rate = pd.read_csv('data/unemployment rate London.csv')\n",
    "unemployment_rate['date'] = pd.to_datetime(unemployment_rate['year'])\n",
    "unemployment_rate = split_date(unemployment_rate, 'date')\n",
    "unemployment_rate = unemployment_rate.drop(columns = 'date')\n",
    "\n",
    "df = merge_custom(df, unemployment_rate, month=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Погода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import meteostat as mt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>country</th>\n",
       "      <th>region</th>\n",
       "      <th>wmo</th>\n",
       "      <th>icao</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>elevation</th>\n",
       "      <th>timezone</th>\n",
       "      <th>hourly_start</th>\n",
       "      <th>hourly_end</th>\n",
       "      <th>daily_start</th>\n",
       "      <th>daily_end</th>\n",
       "      <th>monthly_start</th>\n",
       "      <th>monthly_end</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>03779</th>\n",
       "      <td>London Weather Centre</td>\n",
       "      <td>GB</td>\n",
       "      <td>ENG</td>\n",
       "      <td>03779</td>\n",
       "      <td>EGRB</td>\n",
       "      <td>51.5167</td>\n",
       "      <td>-0.1167</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Europe/London</td>\n",
       "      <td>1992-04-01</td>\n",
       "      <td>2010-02-01</td>\n",
       "      <td>1992-04-03</td>\n",
       "      <td>2010-01-31</td>\n",
       "      <td>2006-01-01</td>\n",
       "      <td>2009-01-01</td>\n",
       "      <td>1104.206082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EGLC0</th>\n",
       "      <td>London / Abbey Wood</td>\n",
       "      <td>GB</td>\n",
       "      <td>ENG</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>EGLC</td>\n",
       "      <td>51.5000</td>\n",
       "      <td>0.1167</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Europe/London</td>\n",
       "      <td>1988-01-29</td>\n",
       "      <td>2024-11-07</td>\n",
       "      <td>2007-09-26</td>\n",
       "      <td>2022-04-27</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>16804.070848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>03672</th>\n",
       "      <td>Northolt</td>\n",
       "      <td>GB</td>\n",
       "      <td>ENG</td>\n",
       "      <td>03672</td>\n",
       "      <td>EGWU</td>\n",
       "      <td>51.5500</td>\n",
       "      <td>-0.4167</td>\n",
       "      <td>38.0</td>\n",
       "      <td>Europe/London</td>\n",
       "      <td>1973-01-01</td>\n",
       "      <td>2024-11-07</td>\n",
       "      <td>1973-01-05</td>\n",
       "      <td>2022-04-25</td>\n",
       "      <td>2005-01-01</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>20652.355896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>03772</th>\n",
       "      <td>London Heathrow Airport</td>\n",
       "      <td>GB</td>\n",
       "      <td>ENG</td>\n",
       "      <td>03772</td>\n",
       "      <td>EGLL</td>\n",
       "      <td>51.4833</td>\n",
       "      <td>-0.4500</td>\n",
       "      <td>24.0</td>\n",
       "      <td>Europe/London</td>\n",
       "      <td>1948-12-01</td>\n",
       "      <td>2024-11-07</td>\n",
       "      <td>1948-12-01</td>\n",
       "      <td>2024-10-30</td>\n",
       "      <td>1948-01-01</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>22624.394649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>03781</th>\n",
       "      <td>Kenley</td>\n",
       "      <td>GB</td>\n",
       "      <td>ENG</td>\n",
       "      <td>03781</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>51.3000</td>\n",
       "      <td>-0.0833</td>\n",
       "      <td>170.0</td>\n",
       "      <td>Europe/London</td>\n",
       "      <td>2018-01-27</td>\n",
       "      <td>2024-11-07</td>\n",
       "      <td>2018-01-28</td>\n",
       "      <td>2022-04-23</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>23369.949135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          name country region    wmo  icao  latitude  \\\n",
       "id                                                                     \n",
       "03779    London Weather Centre      GB    ENG  03779  EGRB   51.5167   \n",
       "EGLC0      London / Abbey Wood      GB    ENG   <NA>  EGLC   51.5000   \n",
       "03672                 Northolt      GB    ENG  03672  EGWU   51.5500   \n",
       "03772  London Heathrow Airport      GB    ENG  03772  EGLL   51.4833   \n",
       "03781                   Kenley      GB    ENG  03781  <NA>   51.3000   \n",
       "\n",
       "       longitude  elevation       timezone hourly_start hourly_end  \\\n",
       "id                                                                   \n",
       "03779    -0.1167        5.0  Europe/London   1992-04-01 2010-02-01   \n",
       "EGLC0     0.1167        5.0  Europe/London   1988-01-29 2024-11-07   \n",
       "03672    -0.4167       38.0  Europe/London   1973-01-01 2024-11-07   \n",
       "03772    -0.4500       24.0  Europe/London   1948-12-01 2024-11-07   \n",
       "03781    -0.0833      170.0  Europe/London   2018-01-27 2024-11-07   \n",
       "\n",
       "      daily_start  daily_end monthly_start monthly_end      distance  \n",
       "id                                                                    \n",
       "03779  1992-04-03 2010-01-31    2006-01-01  2009-01-01   1104.206082  \n",
       "EGLC0  2007-09-26 2022-04-27    2016-01-01  2022-01-01  16804.070848  \n",
       "03672  1973-01-05 2022-04-25    2005-01-01  2022-01-01  20652.355896  \n",
       "03772  1948-12-01 2024-10-30    1948-01-01  2022-01-01  22624.394649  \n",
       "03781  2018-01-28 2022-04-23    2018-01-01  2022-01-01  23369.949135  "
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# посмотрим, какие есть станции в радиусе 50 км от лондона\n",
    "stations = mt.Stations()\n",
    "stations = stations.nearby(lat = 51.5085, lon = -0.1257, radius=100 * 1000)\n",
    "stations = stations.fetch()\n",
    "stations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03779 London Weather Centre (174723, 11)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 174723 entries, 1992-04-01 00:00:00 to 2020-12-31 23:00:00\n",
      "Data columns (total 11 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   temp    174526 non-null  float64\n",
      " 1   dwpt    174426 non-null  float64\n",
      " 2   rhum    174426 non-null  float64\n",
      " 3   prcp    0 non-null       float64\n",
      " 4   snow    0 non-null       float64\n",
      " 5   wdir    142661 non-null  float64\n",
      " 6   wspd    142669 non-null  float64\n",
      " 7   wpgt    19851 non-null   float64\n",
      " 8   pres    173340 non-null  float64\n",
      " 9   tsun    0 non-null       float64\n",
      " 10  coco    22875 non-null   float64\n",
      "dtypes: float64(11)\n",
      "memory usage: 16.0 MB\n",
      "None\n",
      "\n",
      "\n",
      "\n",
      "EGLC0 London / Abbey Wood (197520, 11)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 197520 entries, 1988-01-29 14:00:00 to 2020-12-31 23:00:00\n",
      "Data columns (total 11 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   temp    196962 non-null  float64\n",
      " 1   dwpt    196882 non-null  float64\n",
      " 2   rhum    196882 non-null  float64\n",
      " 3   prcp    0 non-null       float64\n",
      " 4   snow    0 non-null       float64\n",
      " 5   wdir    180359 non-null  float64\n",
      " 6   wspd    196812 non-null  float64\n",
      " 7   wpgt    0 non-null       float64\n",
      " 8   pres    8392 non-null    float64\n",
      " 9   tsun    0 non-null       float64\n",
      " 10  coco    1486 non-null    float64\n",
      "dtypes: float64(11)\n",
      "memory usage: 18.1 MB\n",
      "None\n",
      "\n",
      "\n",
      "\n",
      "03672 Northolt (334894, 11)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 334894 entries, 1979-01-01 09:00:00 to 2020-12-31 23:00:00\n",
      "Data columns (total 11 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   temp    334490 non-null  float64\n",
      " 1   dwpt    334067 non-null  float64\n",
      " 2   rhum    334067 non-null  float64\n",
      " 3   prcp    1 non-null       float64\n",
      " 4   snow    0 non-null       float64\n",
      " 5   wdir    314222 non-null  float64\n",
      " 6   wspd    334543 non-null  float64\n",
      " 7   wpgt    19851 non-null   float64\n",
      " 8   pres    311683 non-null  float64\n",
      " 9   tsun    0 non-null       float64\n",
      " 10  coco    25418 non-null   float64\n",
      "dtypes: float64(11)\n",
      "memory usage: 30.7 MB\n",
      "None\n",
      "\n",
      "\n",
      "\n",
      "03772 London Heathrow Airport (366535, 11)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 366535 entries, 1979-01-01 00:00:00 to 2020-12-31 23:00:00\n",
      "Data columns (total 11 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   temp    366328 non-null  float64\n",
      " 1   dwpt    366176 non-null  float64\n",
      " 2   rhum    366176 non-null  float64\n",
      " 3   prcp    0 non-null       float64\n",
      " 4   snow    67 non-null      float64\n",
      " 5   wdir    359839 non-null  float64\n",
      " 6   wspd    365756 non-null  float64\n",
      " 7   wpgt    20181 non-null   float64\n",
      " 8   pres    362848 non-null  float64\n",
      " 9   tsun    30 non-null      float64\n",
      " 10  coco    26852 non-null   float64\n",
      "dtypes: float64(11)\n",
      "memory usage: 33.6 MB\n",
      "None\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "URLError",
     "evalue": "<urlopen error [Errno 54] Connection reset by peer>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/urllib/request.py:1348\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1348\u001b[0m     h\u001b[38;5;241m.\u001b[39mrequest(req\u001b[38;5;241m.\u001b[39mget_method(), req\u001b[38;5;241m.\u001b[39mselector, req\u001b[38;5;241m.\u001b[39mdata, headers,\n\u001b[1;32m   1349\u001b[0m               encode_chunked\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39mhas_header(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransfer-encoding\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:1298\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_request(method, url, body, headers, encode_chunked)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:1344\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1343\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendheaders(body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:1293\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_output(message_body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:1052\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1052\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(msg)\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1055\u001b[0m \n\u001b[1;32m   1056\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:990\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m--> 990\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[1;32m    991\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:1470\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1468\u001b[0m     server_hostname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n\u001b[0;32m-> 1470\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context\u001b[38;5;241m.\u001b[39mwrap_socket(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock,\n\u001b[1;32m   1471\u001b[0m                                       server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    512\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    514\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msslsocket_class\u001b[38;5;241m.\u001b[39m_create(\n\u001b[1;32m    518\u001b[0m         sock\u001b[38;5;241m=\u001b[39msock,\n\u001b[1;32m    519\u001b[0m         server_side\u001b[38;5;241m=\u001b[39mserver_side,\n\u001b[1;32m    520\u001b[0m         do_handshake_on_connect\u001b[38;5;241m=\u001b[39mdo_handshake_on_connect,\n\u001b[1;32m    521\u001b[0m         suppress_ragged_eofs\u001b[38;5;241m=\u001b[39msuppress_ragged_eofs,\n\u001b[1;32m    522\u001b[0m         server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname,\n\u001b[1;32m    523\u001b[0m         context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    524\u001b[0m         session\u001b[38;5;241m=\u001b[39msession\n\u001b[1;32m    525\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1103\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1104\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:1382\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1381\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[1;32m   1383\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mConnectionResetError\u001b[0m: [Errno 54] Connection reset by peer",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m end \u001b[38;5;241m=\u001b[39m datetime(\u001b[38;5;241m2020\u001b[39m, \u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m31\u001b[39m, \u001b[38;5;241m23\u001b[39m, \u001b[38;5;241m59\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m station \u001b[38;5;129;01min\u001b[39;00m stations\u001b[38;5;241m.\u001b[39mindex:\n\u001b[0;32m----> 5\u001b[0m     weather_hourly \u001b[38;5;241m=\u001b[39m mt\u001b[38;5;241m.\u001b[39mHourly(station, start, end)\n\u001b[1;32m      6\u001b[0m     weather_hourly \u001b[38;5;241m=\u001b[39m weather_hourly\u001b[38;5;241m.\u001b[39mfetch()\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m weather_hourly\u001b[38;5;241m.\u001b[39mfirst_valid_index() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m datetime(\u001b[38;5;241m2000\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m weather_hourly\u001b[38;5;241m.\u001b[39mlast_valid_index() \u001b[38;5;241m==\u001b[39m datetime(\u001b[38;5;241m2020\u001b[39m, \u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m31\u001b[39m, \u001b[38;5;241m23\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/meteostat/interface/hourly.py:146\u001b[0m, in \u001b[0;36mHourly.__init__\u001b[0;34m(self, loc, start, end, timezone, model, flags)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_time(start, end, timezone)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# Initialize time series\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_time_series(loc, start, end, model, flags)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/meteostat/interface/timeseries.py:170\u001b[0m, in \u001b[0;36mTimeSeries._init_time_series\u001b[0;34m(self, loc, start, end, model, flags)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flags \u001b[38;5;241m=\u001b[39m flags\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# Get data for all weather stations\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# Load source flags through map file\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# if flags are explicitly requested or\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# model data is excluded\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/meteostat/interface/meteodata.py:127\u001b[0m, in \u001b[0;36mMeteoData._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m     datasets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_datasets()\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;66;03m# Data Processings\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m processing_handler(\n\u001b[1;32m    128\u001b[0m         datasets, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocesses, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreads\n\u001b[1;32m    129\u001b[0m     )\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# Empty DataFrame\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_types])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/meteostat/core/loader.py:59\u001b[0m, in \u001b[0;36mprocessing_handler\u001b[0;34m(datasets, load, cores, threads)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Single-thread processing\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[0;32m---> 59\u001b[0m         output\u001b[38;5;241m.\u001b[39mappend(load(\u001b[38;5;241m*\u001b[39mdataset))\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Remove empty DataFrames\u001b[39;00m\n\u001b[1;32m     62\u001b[0m filtered \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m df: df\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, output))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/meteostat/interface/meteodata.py:59\u001b[0m, in \u001b[0;36mMeteoData._load_data\u001b[0;34m(self, station, year)\u001b[0m\n\u001b[1;32m     54\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_pickle(path)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m \n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# Get data from Meteostat\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m     df \u001b[38;5;241m=\u001b[39m load_handler(\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint, file, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_columns, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_types, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_dates\n\u001b[1;32m     61\u001b[0m     )\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;66;03m# Validate and prepare data for further processing\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgranularity \u001b[38;5;241m==\u001b[39m Granularity\u001b[38;5;241m.\u001b[39mNORMALS \u001b[38;5;129;01mand\u001b[39;00m df\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;66;03m# Add weather station ID\u001b[39;00m\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;66;03m# pylint: disable=unsupported-assignment-operation\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/meteostat/core/loader.py:82\u001b[0m, in \u001b[0;36mload_handler\u001b[0;34m(endpoint, path, columns, types, parse_dates, coerce_dates)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03mLoad a single CSV file into a DataFrame\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m \n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# Read CSV file from Meteostat endpoint\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[1;32m     83\u001b[0m         endpoint \u001b[38;5;241m+\u001b[39m path,\n\u001b[1;32m     84\u001b[0m         compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     85\u001b[0m         names\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m     86\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mtypes,\n\u001b[1;32m     87\u001b[0m         parse_dates\u001b[38;5;241m=\u001b[39mparse_dates,\n\u001b[1;32m     88\u001b[0m     )\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;66;03m# Force datetime conversion\u001b[39;00m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m coerce_dates:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1706\u001b[0m     f,\n\u001b[1;32m   1707\u001b[0m     mode,\n\u001b[1;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1714\u001b[0m )\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:718\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    715\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[1;32m    717\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[0;32m--> 718\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m _get_filepath_or_buffer(\n\u001b[1;32m    719\u001b[0m     path_or_buf,\n\u001b[1;32m    720\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[1;32m    721\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m    722\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m    723\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    724\u001b[0m )\n\u001b[1;32m    726\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[1;32m    727\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:372\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[1;32m    371\u001b[0m req_info \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[0;32m--> 372\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m urlopen(req_info) \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[1;32m    373\u001b[0m     content_encoding \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    375\u001b[0m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:274\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03mthe stdlib.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m opener\u001b[38;5;241m.\u001b[39mopen(url, data, timeout)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/urllib/request.py:519\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    516\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[1;32m    518\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[0;32m--> 519\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open(req, data)\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[1;32m    522\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/urllib/request.py:536\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    535\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[0;32m--> 536\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_chain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_open, protocol, protocol \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    537\u001b[0m                           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_open\u001b[39m\u001b[38;5;124m'\u001b[39m, req)\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/urllib/request.py:1391\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[0;32m-> 1391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_open(http\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mHTTPSConnection, req,\n\u001b[1;32m   1392\u001b[0m         context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context, check_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_hostname)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/urllib/request.py:1351\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m         h\u001b[38;5;241m.\u001b[39mrequest(req\u001b[38;5;241m.\u001b[39mget_method(), req\u001b[38;5;241m.\u001b[39mselector, req\u001b[38;5;241m.\u001b[39mdata, headers,\n\u001b[1;32m   1349\u001b[0m                   encode_chunked\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39mhas_header(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransfer-encoding\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[0;32m-> 1351\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[1;32m   1352\u001b[0m     r \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[0;31mURLError\u001b[0m: <urlopen error [Errno 54] Connection reset by peer>"
     ]
    }
   ],
   "source": [
    "# посмотрим сколько пропусков на каждой станции\n",
    "start = datetime(1979, 1, 1)\n",
    "end = datetime(2020, 12, 31, 23, 59)\n",
    "for station in stations.index:\n",
    "    weather_hourly = mt.Hourly(station, start, end)\n",
    "    weather_hourly = weather_hourly.fetch()\n",
    "    if weather_hourly.first_valid_index() <= datetime(2000, 1, 1) and weather_hourly.last_valid_index() == datetime(2020, 12, 31, 23):\n",
    "        print(station, stations.loc[station][0], weather_hourly.shape)\n",
    "        print(weather_hourly.info())\n",
    "        print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "увы, но prcp никто не отмечал и coco на каждой станции мало, не хватит для нормальной обучающей выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# возьмем London Heathrow Airport\n",
    "start = datetime(1979, 1, 1)\n",
    "end = datetime(2020, 12, 31, 23, 59)\n",
    "weather_hourly = mt.Hourly('03772', start, end)\n",
    "weather_hourly = weather_hourly.fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_hourly = weather_hourly.reset_index()[['temp', 'dwpt', 'rhum', 'wdir', 'wspd', 'pres', 'time']]\n",
    "weather_hourly['time'] = pd.to_datetime(weather_hourly['time'])\n",
    "weather_hourly = split_date(weather_hourly, 'time', hour=True)\n",
    "weather_hourly = weather_hourly.drop(columns=['time'])\n",
    "df = merge_custom(df, weather_hourly, hour=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "# дневные prcp и snow не будем добавлять, я пока еще поищу \n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Время рассвета и заката"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astral import LocationInfo\n",
    "from astral.sun import sun\n",
    "from astral.location import Location\n",
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.date_range(start=\"1979-01-01\", end=\"2020-12-31\", freq=\"D\")\n",
    "\n",
    "city = LocationInfo(\"London\", \"England\")\n",
    "sun_data = []\n",
    "for date in dates:\n",
    "    s = sun(city.observer, date=date, tzinfo=city.timezone)\n",
    "    sun_data.append({\n",
    "        'date' : date,\n",
    "        'sunrise' : s['sunrise'],\n",
    "        'sunset' : s['sunset']\n",
    "    })\n",
    "sun_data = pd.DataFrame(sun_data)\n",
    "\n",
    "sun_data['sunrise'] = sun_data['sunrise'].dt.time\n",
    "sun_data['sunset'] = sun_data['sunset'].dt.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "sun_data = split_date(sun_data, 'date', day=True)\n",
    "sun_data.drop(columns='date', inplace=True)\n",
    "df = merge_custom(df, sun_data, day=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['darkness'] = (~((df['sunrise'] < df['date'].dt.time) & (df['date'].dt.time < df['sunset']))).astype(int)\n",
    "df.drop(columns=['sunrise', 'sunset'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Фазы луны"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astral import moon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "moon_data = []\n",
    "for date in dates:\n",
    "    m = moon.phase(date=date)\n",
    "    moon_data.append({\n",
    "        'date' : date,\n",
    "        'moon_phase_value' : m,\n",
    "    })\n",
    "moon_data = pd.DataFrame(moon_data)\n",
    "\n",
    "def get_moon_phase(moon_phase_value):\n",
    "    if 0 <= moon_phase_value < 7:\n",
    "        return 'New Moon'\n",
    "    elif 7 <= moon_phase_value < 14:\n",
    "        return 'First Quarter'\n",
    "    elif 14 <= moon_phase_value < 21:\n",
    "        return 'Full Moon'\n",
    "    elif 21 <= moon_phase_value < 28:\n",
    "        return 'Last Quarter'\n",
    "\n",
    "moon_data['moon_phase'] = moon_data['moon_phase_value'].apply(get_moon_phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "moon_data = split_date(moon_data, 'date', day=True)\n",
    "moon_data.drop(columns=['date'], inplace=True)\n",
    "df = merge_custom(df, moon_data, day=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сохраняем Итог"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pressure</th>\n",
       "      <th>cloud_cover</th>\n",
       "      <th>evgeniy_gennadievich_beer</th>\n",
       "      <th>sunshine</th>\n",
       "      <th>tarot</th>\n",
       "      <th>min_temp</th>\n",
       "      <th>date</th>\n",
       "      <th>max_temp</th>\n",
       "      <th>snow_depth</th>\n",
       "      <th>mean_temp</th>\n",
       "      <th>...</th>\n",
       "      <th>unemployment_rate</th>\n",
       "      <th>temp</th>\n",
       "      <th>dwpt</th>\n",
       "      <th>rhum</th>\n",
       "      <th>wdir</th>\n",
       "      <th>wspd</th>\n",
       "      <th>pres</th>\n",
       "      <th>darkness</th>\n",
       "      <th>moon_phase_value</th>\n",
       "      <th>moon_phase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101860.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>('The Devil', 'The World')</td>\n",
       "      <td>4.8</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>...</td>\n",
       "      <td>5.9</td>\n",
       "      <td>6.7</td>\n",
       "      <td>5.2</td>\n",
       "      <td>90.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>18.4</td>\n",
       "      <td>1024.1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.911222</td>\n",
       "      <td>New Moon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101860.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>('The High Priestess', 'Justice')</td>\n",
       "      <td>4.8</td>\n",
       "      <td>2017-01-01 01:00:00</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>...</td>\n",
       "      <td>5.9</td>\n",
       "      <td>6.2</td>\n",
       "      <td>5.2</td>\n",
       "      <td>93.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1022.7</td>\n",
       "      <td>1</td>\n",
       "      <td>2.911222</td>\n",
       "      <td>New Moon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101860.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>('The Empress', 'The Magician')</td>\n",
       "      <td>4.8</td>\n",
       "      <td>2017-01-01 02:00:00</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>...</td>\n",
       "      <td>5.9</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>1021.9</td>\n",
       "      <td>1</td>\n",
       "      <td>2.911222</td>\n",
       "      <td>New Moon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101860.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>('The Lovers', 'The Star')</td>\n",
       "      <td>4.8</td>\n",
       "      <td>2017-01-01 03:00:00</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>...</td>\n",
       "      <td>5.9</td>\n",
       "      <td>5.7</td>\n",
       "      <td>4.8</td>\n",
       "      <td>94.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1020.7</td>\n",
       "      <td>1</td>\n",
       "      <td>2.911222</td>\n",
       "      <td>New Moon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101860.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>('The Empress', 'The Star')</td>\n",
       "      <td>4.8</td>\n",
       "      <td>2017-01-01 04:00:00</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>...</td>\n",
       "      <td>5.9</td>\n",
       "      <td>5.6</td>\n",
       "      <td>4.6</td>\n",
       "      <td>93.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>14.8</td>\n",
       "      <td>1019.6</td>\n",
       "      <td>1</td>\n",
       "      <td>2.911222</td>\n",
       "      <td>New Moon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35059</th>\n",
       "      <td>100500.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>('The Tower', 'The Star')</td>\n",
       "      <td>-3.1</td>\n",
       "      <td>2020-12-31 19:00:00</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>...</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>91.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1007.6</td>\n",
       "      <td>1</td>\n",
       "      <td>15.277889</td>\n",
       "      <td>Full Moon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35060</th>\n",
       "      <td>100500.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>('The Magician', 'The Sun')</td>\n",
       "      <td>-3.1</td>\n",
       "      <td>2020-12-31 20:00:00</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>...</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>90.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1008.0</td>\n",
       "      <td>1</td>\n",
       "      <td>15.277889</td>\n",
       "      <td>Full Moon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35061</th>\n",
       "      <td>100500.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>('The High Priestess', 'The Lovers')</td>\n",
       "      <td>-3.1</td>\n",
       "      <td>2020-12-31 21:00:00</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>...</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>90.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1008.4</td>\n",
       "      <td>1</td>\n",
       "      <td>15.277889</td>\n",
       "      <td>Full Moon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35062</th>\n",
       "      <td>100500.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>('The Hierophant', 'The Hermit')</td>\n",
       "      <td>-3.1</td>\n",
       "      <td>2020-12-31 22:00:00</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>...</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>90.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>1008.6</td>\n",
       "      <td>1</td>\n",
       "      <td>15.277889</td>\n",
       "      <td>Full Moon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35063</th>\n",
       "      <td>100500.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>('The Hierophant', 'The Moon')</td>\n",
       "      <td>-3.1</td>\n",
       "      <td>2020-12-31 23:00:00</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>...</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>91.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>1009.1</td>\n",
       "      <td>1</td>\n",
       "      <td>15.277889</td>\n",
       "      <td>Full Moon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35064 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pressure  cloud_cover  evgeniy_gennadievich_beer  sunshine  \\\n",
       "0      101860.0            8                        0.0       0.0   \n",
       "1      101860.0            8                        0.0       0.0   \n",
       "2      101860.0            8                        0.0       0.0   \n",
       "3      101860.0            8                        0.0       0.0   \n",
       "4      101860.0            8                        0.0       0.0   \n",
       "...         ...          ...                        ...       ...   \n",
       "35059  100500.0            7                        0.0       1.3   \n",
       "35060  100500.0            7                        0.1       1.3   \n",
       "35061  100500.0            7                        0.1       1.3   \n",
       "35062  100500.0            7                        0.1       1.3   \n",
       "35063  100500.0            7                        0.1       1.3   \n",
       "\n",
       "                                      tarot  min_temp                date  \\\n",
       "0                ('The Devil', 'The World')       4.8 2017-01-01 00:00:00   \n",
       "1         ('The High Priestess', 'Justice')       4.8 2017-01-01 01:00:00   \n",
       "2           ('The Empress', 'The Magician')       4.8 2017-01-01 02:00:00   \n",
       "3                ('The Lovers', 'The Star')       4.8 2017-01-01 03:00:00   \n",
       "4               ('The Empress', 'The Star')       4.8 2017-01-01 04:00:00   \n",
       "...                                     ...       ...                 ...   \n",
       "35059             ('The Tower', 'The Star')      -3.1 2020-12-31 19:00:00   \n",
       "35060           ('The Magician', 'The Sun')      -3.1 2020-12-31 20:00:00   \n",
       "35061  ('The High Priestess', 'The Lovers')      -3.1 2020-12-31 21:00:00   \n",
       "35062      ('The Hierophant', 'The Hermit')      -3.1 2020-12-31 22:00:00   \n",
       "35063        ('The Hierophant', 'The Moon')      -3.1 2020-12-31 23:00:00   \n",
       "\n",
       "       max_temp  snow_depth  mean_temp  ... unemployment_rate  temp  dwpt  \\\n",
       "0           7.5         0.0        7.5  ...               5.9   6.7   5.2   \n",
       "1           7.5         0.0        7.5  ...               5.9   6.2   5.2   \n",
       "2           7.5         0.0        7.5  ...               5.9   6.0   5.0   \n",
       "3           7.5         0.0        7.5  ...               5.9   5.7   4.8   \n",
       "4           7.5         0.0        7.5  ...               5.9   5.6   4.6   \n",
       "...         ...         ...        ...  ...               ...   ...   ...   \n",
       "35059       1.5         0.0       -0.8  ...               7.5   0.4  -0.9   \n",
       "35060       1.5         0.0       -0.8  ...               7.5   0.7  -0.8   \n",
       "35061       1.5         0.0       -0.8  ...               7.5   0.8  -0.7   \n",
       "35062       1.5         0.0       -0.8  ...               7.5   1.0  -0.5   \n",
       "35063       1.5         0.0       -0.8  ...               7.5   0.7  -0.6   \n",
       "\n",
       "       rhum   wdir  wspd    pres  darkness  moon_phase_value  moon_phase  \n",
       "0      90.0  200.0  18.4  1024.1         1          2.911222    New Moon  \n",
       "1      93.0  210.0  13.0  1022.7         1          2.911222    New Moon  \n",
       "2      93.0  210.0  16.6  1021.9         1          2.911222    New Moon  \n",
       "3      94.0  200.0  13.0  1020.7         1          2.911222    New Moon  \n",
       "4      93.0  210.0  14.8  1019.6         1          2.911222    New Moon  \n",
       "...     ...    ...   ...     ...       ...               ...         ...  \n",
       "35059  91.0  330.0   3.6  1007.6         1         15.277889   Full Moon  \n",
       "35060  90.0  320.0   1.8  1008.0         1         15.277889   Full Moon  \n",
       "35061  90.0  360.0   3.6  1008.4         1         15.277889   Full Moon  \n",
       "35062  90.0  320.0   5.4  1008.6         1         15.277889   Full Moon  \n",
       "35063  91.0   20.0   5.4  1009.1         1         15.277889   Full Moon  \n",
       "\n",
       "[35064 rows x 45 columns]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.to_csv('./data/train_udp.csv', index = False)\n",
    "df.to_csv('./data/test_udp.csv', index = False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env3.10",
   "language": "python",
   "name": "env3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
